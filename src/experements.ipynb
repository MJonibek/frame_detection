{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "\n",
    "def make_dataframe(input_folder, labels_folder=None):\n",
    "    text = []\n",
    "\n",
    "    for fil in tqdm(filter(lambda x: x.endswith('.txt'),\n",
    "                           os.listdir(input_folder))):\n",
    "        iD, txt = fil[7:].split('.')[0], open(os.path.join(input_folder, fil),\n",
    "                                              'r', encoding='utf-8').read()\n",
    "        text.append((iD, txt))\n",
    "\n",
    "    df_text = pd.DataFrame(text, columns=['id','text']).set_index('id')\n",
    "    df = df_text\n",
    "\n",
    "    if labels_folder:\n",
    "        labels = pd.read_csv(labels_folder, sep='\\t', header=None)\n",
    "        labels = labels.rename(columns={0:'id', 1:'frames'})\n",
    "        labels.id = labels.id.apply(str)\n",
    "        labels = labels.set_index('id')\n",
    "\n",
    "        df = labels.join(df_text)[['text', 'frames']]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# def read_data(data):\n",
    "#     X_data = data['text'].values\n",
    "#     Y_data = data['frames'].str.split(',').values\n",
    "#     Y_data = encoder.fit_transform(Y_data)\n",
    "#\n",
    "#     return (X_data, Y_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "mispell_dict = {\"ain’t\": \"is not\", \"aren’t\": \"are not\",\"can’t\": \"cannot\", \"’cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\", \"didn’t\": \"did not\",  \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he would\",\"he’ll\": \"he will\", \"he’s\": \"he is\", \"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"how’s\": \"how is\",  \"I’d\": \"I would\", \"I’d’ve\": \"I would have\", \"I’ll\": \"I will\", \"I’ll’ve\": \"I will have\",\"I’m\": \"I am\", \"I’ve\": \"I have\", \"i’d\": \"i would\", \"i’d’ve\": \"i would have\", \"i’ll\": \"i will\",  \"i’ll’ve\": \"i will have\",\"i’m\": \"i am\", \"i’ve\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\", \"it’d’ve\": \"it would have\", \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\",\"it’s\": \"it is\", \"let’s\": \"let us\", \"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\",\"mightn’t\": \"might not\",\"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\", \"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\",\"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\", \"shan’t\": \"shall not\", \"sha’n’t\": \"shall not\", \"shan’t’ve\": \"shall not have\", \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"she’ll\": \"she will\", \"she’ll’ve\": \"she will have\", \"she’s\": \"she is\", \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\", \"so’ve\": \"so have\",\"so’s\": \"so as\", \"this’s\": \"this is\",\"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"that’s\": \"that is\", \"there’d\": \"there would\", \"there’d’ve\": \"there would have\", \"there’s\": \"there is\", \"here’s\": \"here is\",\"they’d\": \"they would\", \"they’d’ve\": \"they would have\", \"they’ll\": \"they will\", \"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\", \"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\", \"what’ll\": \"what will\", \"what’ll’ve\": \"what will have\", \"what’re\": \"what are\",  \"what’s\": \"what is\", \"what’ve\": \"what have\", \"when’s\": \"when is\", \"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’s\": \"where is\", \"where’ve\": \"where have\", \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\", \"who’s\": \"who is\", \"who’ve\": \"who have\", \"why’s\": \"why is\", \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\", \"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\",\"y’all’d’ve\": \"you all would have\",\"y’all’re\": \"you all are\",\"y’all’ve\": \"you all have\",\"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"you’ll’ve\": \"you will have\", \"you’re\": \"you are\", \"you’ve\": \"you have\", \"she`s\": \"she is\", \"\\n\": \" \"}\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    for key in mispell_dict.keys():\n",
    "        text = text.replace(key, mispell_dict[key])\n",
    "    return text\n",
    "\n",
    "# train_df = pd.DataFrame({'text': train_data.text})\n",
    "# train_df.text = train_df['text'].progress_apply(lambda x: replace_typical_misspell(x.lower()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "433it [00:00, 7715.33it/s]\n",
      "433it [00:00, 35676.22it/s]\n",
      "433it [00:00, 35558.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train = '../data/en/train-articles-subtask-2'\n",
    "train_label = '../data/en/train-labels-subtask-2.txt'\n",
    "test = '../data/en/dev-articles-subtask-2'\n",
    "test_label = '../data/en/dev-labels-subtask-2.txt'\n",
    "\n",
    "data = make_dataframe(train, train_label)\n",
    "train_data = make_dataframe(train, train_label)[:400]\n",
    "dev_data = make_dataframe(train, train_label)[400:]\n",
    "\n",
    "X_train = data['text'].values[:400]\n",
    "X_dev = data['text'].values[400:]\n",
    "\n",
    "encoder = MultiLabelBinarizer()\n",
    "\n",
    "Y = data['frames'].str.split(',').values\n",
    "Y_1 = encoder.fit_transform(Y)[:, [0, 2, 3, 5, 6, 9, 11, 12]]\n",
    "Y_2 = encoder.fit_transform(Y)[:, [1, 4, 7, 8, 10, 13]]\n",
    "Y_train = Y_1[:400]\n",
    "Y_dev = Y_1[400:]\n",
    "Y_2_train = Y_2[:400]\n",
    "Y_2_dev = Y_2[400:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 9503.36it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = make_dataframe(test, test_label)\n",
    "\n",
    "X_test = test_data['text'].values\n",
    "\n",
    "Y_test = test_data['frames'].str.split(',').values\n",
    "Y_2_test = encoder.fit_transform(Y_test)[:, [1, 4, 7, 8, 10, 13]]\n",
    "Y_test = encoder.fit_transform(Y_test)[:,[0, 2, 3, 5, 6, 9, 11, 12]]\n",
    "\n",
    "# Y_test = encoder.fit_transform(Y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "X_train = [replace_typical_misspell(x.lower()) for x in X_train]\n",
    "X_dev = [replace_typical_misspell(x.lower()) for x in X_dev]\n",
    "X_test = [replace_typical_misspell(x.lower()) for x in X_test]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, XLNetTokenizerFast\n",
    "\n",
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Digits, Whitespace, Punctuation\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "\n",
    "# train a tokenizer, initialize WordLevel tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "# we first define a normalizer applied before tokenization\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "# pre-tokenizer defines a \"preprocessing\" before the tokenization.\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(),\n",
    "                                                   Digits(individual_digits=True)])\n",
    "# training a tokenizer is effectively building a vocabulary in this case\n",
    "trainer = WordLevelTrainer(vocab_size=50000, special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "tokenizer.train_from_iterator(train_data.text.values, trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "#load a tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\"\n",
    ")\n",
    "\n",
    "# tokenizer = XLNetTokenizerFast(\n",
    "#     tokenizer_file=\"tokenizer.json\",\n",
    "#     unk_token=\"[UNK]\",\n",
    "#     pad_token=\"[PAD]\"\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "class SemEvalTask3Subtask2(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_token_len=512, labels_1=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        self.labels_1=labels_1\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        label_1 = self.labels_1[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_token_len,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            input_ids=encoding,\n",
    "            label=torch.FloatTensor(label),\n",
    "            label_1=torch.FloatTensor(label_1),\n",
    "            label_1_output=torch.FloatTensor(label_1)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "train_dataset = SemEvalTask3Subtask2(\n",
    "    X_train, Y_train, tokenizer, labels_1=Y_2_train\n",
    ")\n",
    "dev_dataset = SemEvalTask3Subtask2(\n",
    "    X_dev, Y_dev, tokenizer, labels_1=Y_2_dev\n",
    ")\n",
    "test_dataset = SemEvalTask3Subtask2(\n",
    "    X_test, Y_test, tokenizer, labels_1=Y_2_test\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 4289,  6890,  2867,    74,    12,   170,   142,  1825,  3939,     5,\n           576,   566,    11,   271,    16,   176, 13156,  4289,  6890,    37,\n           872,    74,    12,    14,   142,  1825,  3939,    15,     5,   178,\n           584,  1060,   576,   566,     3,   112,   969,    83,    26,  1533,\n            32,  1509,   110,  1273,  1308,   276,   296,     7,   315,    55,\n            11,    13,  1040,   367,     4,    20,   507,  2649,     3,     2,\n           255,   211,   957,    10,   566,    51,  1533,    32,  1509,   110,\n             2,   445,     7,    10,    26,    73,    29,    27,  7240,   103,\n            86,  2676,   100,    55,    54,   541,   836,   581,     8,    32,\n           367,     4,   353,   480,     2,  7505,     6,  7903,  1641,    28,\n             2,  1065,    20,     2,  4402,   176,     3,  4289,  6890,   872,\n            10,    76,  8499,    25,     2,    98,    95,    16,   144,    16,\n           291,  1814,   108,    31,    63,   109,    74,   274,    89,  1825,\n          9282,     4,    14,    70,    34,   446,   125,   853,  3112,     3,\n            34,   446,   142,  1825,  3939,     5,   576,   566,     3,    15,\n            74,   307,    20,   348,     4,    14,    34,   137,    18,    12,\n            29,  1601,     5,    27,     3,    45,    10,   206,    29,  1050,\n            34,    49,    29,  3523,     9,  5468,     6,    89,   767,   118,\n          1953,     4,    15,    70,    34,   446,   125,   853,  3112,     3,\n            34,   446,   142,  1825,  3939,     5,   576,   566,     4,    34,\n           137,    18,    12,    29,  1601,     5,    27,     3,    45,    10,\n           206,    29,  1050,    34,    49,    29,  3523,     9,  5468,     6,\n            89,   767,   118,  1953,     4,    72,  4289,  6890,    53,   265,\n         13015,    52,   843,    30,    65,     3,    30,    19,    22,    66,\n           169,   398,     3,     2,   729, 16212,   777,   453,  2672,  7514,\n           566,     9,    14,  1793, 11829,   207,     3,    15,   143,  3095,\n           743,    10,    26,    73,   752,  6045,     2,    92,    11,    13,\n           819,   315,    55,  3123,     4,    20,   530,     6,   406,  1308,\n            54,   120,   837,  8425,     4,    14,  1793, 11829,   207,     5,\n           576,   566,     3,     7,    17,    89,  1471,    32,  1034,  2282,\n             5, 13834,     3,  3123,     4,     3,    15,    74,   307,     4,\n            14,    10,    73,   203,    24, 10274,   463,  2131,     4,    15,\n           566,    11,    13,  1034,   147,    37,    29,   353,    63,   369,\n             3,  1026,   266,   260,  3028,  3684,    43,    20,   507,    10,\n            26,   129,  1617,    99,  4310,    23,   752,    23,    24,  1488,\n             4,    14,    34,   446,  4519,     2,   147,     7,  9221,    10,\n            34,   108,    27,     8,     9,   675,     5,  5073,    46,     6,\n             2,   584,  1060,    11,    13,  2088,  3964,    23,   752,    23,\n            24,  1488,     3,    15,  3684,   307,     8,    32,   248,     2,\n           611,   857,     7,   340,    20,     2,   132,     7,   322,  1085,\n          3960,     4,   794,  5116, 10506,    20,   355,     3,   348,    38,\n           265,  5116,   219, 10506,     3,    54,   205,    89,    38, 12598,\n           265,  1682,     4,   408,     4,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]])"
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['input_ids']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "class CNNClassifier_1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 output_size,\n",
    "                 embedding_size=300,\n",
    "                 in_channels=1,\n",
    "                 out_channels=100,\n",
    "                 kernel_sizes=[3,4,5]):\n",
    "        super(CNNClassifier_1, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(self.in_channels, self.out_channels,\n",
    "                       (kernel_size, self.embedding_size))\n",
    "             for kernel_size in self.kernel_sizes])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(self.kernel_sizes) * self.out_channels,\n",
    "                             self.output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(torch.squeeze(x))  # (batch_size, sequence_length, embedding_size)\n",
    "        x = x.unsqueeze(1)  # (batch_size, in_channels, sequence_length, embedding_size)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, out_channels, embedding_size), ...]*len(kernel_sizes)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(batch_size, out_channels), ...]*len(kernel_sizes)\n",
    "        x = torch.cat(x, 1)  # (batch_size, len(kernel_sizes)*out_channels)\n",
    "        x = self.dropout(x)  # (batch_size, len(kernel_sizes)*out_channels)\n",
    "        y = self.fc1(x)  # (batch_size, output_size)\n",
    "        return y\n",
    "\n",
    "    def predict(self, x, threshold=0.5):\n",
    "        preds = self.sigmoid(self.forward(x))\n",
    "        preds = np.array(preds.cpu() > threshold, dtype=float)\n",
    "        return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "class CNNClassifier_2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 output_size,\n",
    "                 embedding_size=300,\n",
    "                 in_channels=1,\n",
    "                 out_channels=100,\n",
    "                 kernel_sizes=[3,4,5]):\n",
    "        super(CNNClassifier_2, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.output2=50\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.input2 = nn.Linear(6, self.output2)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(self.in_channels, self.out_channels,\n",
    "                       (kernel_size, self.embedding_size))\n",
    "             for kernel_size in self.kernel_sizes])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(self.kernel_sizes) * self.out_channels+self.output2,\n",
    "                             self.output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = self.embed(torch.squeeze(x))  # (batch_size, sequence_length, embedding_size)\n",
    "        input2 = self.input2(labels)\n",
    "        x = x.unsqueeze(1)  # (batch_size, in_channels, sequence_length, embedding_size)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, out_channels, embedding_size), ...]*len(kernel_sizes)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(batch_size, out_channels), ...]*len(kernel_sizes)\n",
    "        x = torch.cat(x, 1)  # (batch_size, len(kernel_sizes)*out_channels)\n",
    "        x = self.dropout(x)  # (batch_size, len(kernel_sizes)*out_channels)\n",
    "        combined = torch.cat((x.view(x.size(0), -1),\n",
    "                              input2.view(input2.size(0), -1)), dim=1)\n",
    "        y = self.fc1(combined)  # (batch_size, output_size)\n",
    "        return y\n",
    "\n",
    "    def predict(self, x, labels, threshold=0.5):\n",
    "        preds = self.sigmoid(self.forward(x, labels))\n",
    "        preds = np.array(preds.cpu() > threshold, dtype=float)\n",
    "        return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "class RnnType:\n",
    "    GRU = 1\n",
    "    LSTM = 2\n",
    "\n",
    "class AttentionModel:\n",
    "    NONE = 0\n",
    "    DOT = 1\n",
    "    GENERAL = 2\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, data_dict):\n",
    "        for k, v in data_dict.items():\n",
    "            exec(\"self.%s=%s\" % (k, v))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, device, method, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.concat_linear = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "        if self.method == AttentionModel.GENERAL:\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, rnn_outputs, final_hidden_state):\n",
    "        # rnn_output.shape:         (batch_size, seq_len, hidden_size)\n",
    "        # final_hidden_state.shape: (batch_size, hidden_size)\n",
    "        # NOTE: hidden_size may also reflect bidirectional hidden states (hidden_size = num_directions * hidden_dim)\n",
    "        batch_size, seq_len, _ = rnn_outputs.shape\n",
    "        if self.method == AttentionModel.DOT:\n",
    "            attn_weights = torch.bmm(rnn_outputs, final_hidden_state.unsqueeze(2))\n",
    "        elif self.method == AttentionModel.GENERAL:\n",
    "            attn_weights = self.attn(rnn_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "            attn_weights = torch.bmm(attn_weights, final_hidden_state.unsqueeze(2))\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"[Error] Unknown AttentionModel.\")\n",
    "\n",
    "        attn_weights = torch.softmax(attn_weights.squeeze(2), dim=1)\n",
    "\n",
    "        context = torch.bmm(rnn_outputs.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        attn_hidden = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state), dim=1)))\n",
    "\n",
    "        return attn_hidden, attn_weights\n",
    "\n",
    "\n",
    "class RnnClassifier(nn.Module):\n",
    "    def __init__(self, device, params):\n",
    "        super(RnnClassifier, self).__init__()\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding layer\n",
    "        self.word_embeddings = nn.Embedding(self.params.vocab_size, self.params.embed_dim)\n",
    "\n",
    "        # Calculate number of directions\n",
    "        self.num_directions = 2 if self.params.bidirectional == True else 1\n",
    "\n",
    "        self.linear_dims = [self.params.rnn_hidden_dim * self.num_directions] + self.params.linear_dims\n",
    "        self.linear_dims.append(self.params.label_size)\n",
    "\n",
    "        # RNN layer\n",
    "        rnn = None\n",
    "        if self.params.rnn_type == RnnType.GRU:\n",
    "            rnn = nn.GRU\n",
    "        elif self.params.rnn_type == RnnType.LSTM:\n",
    "            rnn = nn.LSTM\n",
    "        else:\n",
    "            raise Exception(\"[Error] Unknown RnnType. Currently supported: RnnType.GRU=1, RnnType.LSTM=2\")\n",
    "        self.rnn = rnn(self.params.embed_dim,\n",
    "                       self.params.rnn_hidden_dim,\n",
    "                       num_layers=self.params.num_layers,\n",
    "                       bidirectional=self.params.bidirectional,\n",
    "                       dropout=self.params.dropout,\n",
    "                       batch_first=False)\n",
    "\n",
    "\n",
    "        # Define set of fully connected layers (Linear Layer + Activation Layer) * #layers\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(0, len(self.linear_dims)-1):\n",
    "            if self.params.dropout > 0.0:\n",
    "                self.linears.append(nn.Dropout(p=self.params.dropout))\n",
    "            linear_layer = nn.Linear(self.linear_dims[i], self.linear_dims[i+1])\n",
    "            self.init_weights(linear_layer)\n",
    "            self.linears.append(linear_layer)\n",
    "            if i == len(self.linear_dims) - 1:\n",
    "                break  # no activation after output layer!!!\n",
    "            self.linears.append(nn.ReLU())\n",
    "\n",
    "        self.hidden = None\n",
    "\n",
    "        # Choose attention model\n",
    "        if self.params.attention_model != AttentionModel.NONE:\n",
    "            self.attn = Attention(self.device, self.params.attention_model, self.params.rnn_hidden_dim * self.num_directions)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.params.rnn_type == RnnType.GRU:\n",
    "            return torch.zeros(self.params.num_layers * self.num_directions, batch_size, self.params.rnn_hidden_dim).to(self.device)\n",
    "        elif self.params.rnn_type == RnnType.LSTM:\n",
    "            return (torch.zeros(self.params.num_layers * self.num_directions, batch_size, self.params.rnn_hidden_dim).to(self.device),\n",
    "                    torch.zeros(self.params.num_layers * self.num_directions, batch_size, self.params.rnn_hidden_dim).to(self.device))\n",
    "        else:\n",
    "            raise Exception('Unknown rnn_type. Valid options: \"gru\", \"lstm\"')\n",
    "\n",
    "    # def freeze_layer(self, layer):\n",
    "    #     for param in layer.parameters():\n",
    "    #         param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, ems = inputs.shape\n",
    "\n",
    "        # Push through embedding layer\n",
    "        X = self.word_embeddings(torch.squeeze(inputs)).transpose(0, 1)\n",
    "\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        # Push through RNN layer\n",
    "        rnn_output, self.hidden = self.rnn(X, self.hidden)\n",
    "\n",
    "        # Extract last hidden state\n",
    "        final_state = None\n",
    "        if self.params.rnn_type == RnnType.GRU:\n",
    "            final_state = self.hidden.view(self.params.num_layers, self.num_directions, batch_size, self.params.rnn_hidden_dim)[-1]\n",
    "        elif self.params.rnn_type == RnnType.LSTM:\n",
    "            final_state = self.hidden[0].view(self.params.num_layers, self.num_directions, batch_size, self.params.rnn_hidden_dim)[-1]\n",
    "        # Handle directions\n",
    "        final_hidden_state = None\n",
    "        if self.num_directions == 1:\n",
    "            final_hidden_state = final_state.squeeze(0)\n",
    "        elif self.num_directions == 2:\n",
    "            h_1, h_2 = final_state[0], final_state[1]\n",
    "            final_hidden_state = torch.cat((h_1, h_2), 1)  # Concatenate both states\n",
    "\n",
    "        # Push through attention layer\n",
    "        if self.params.attention_model != AttentionModel.NONE:\n",
    "            rnn_output = rnn_output.permute(1, 0, 2)  #\n",
    "            X = self.attn(rnn_output, final_hidden_state)[0]\n",
    "        else:\n",
    "            X = final_hidden_state\n",
    "\n",
    "        # Push through linear layers\n",
    "        for l in self.linears:\n",
    "            X = l(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            # print(\"Initialize layer with nn.init.xavier_uniform_: {}\".format(layer))\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(0.01)\n",
    "\n",
    "    def predict(self, x, threshold=0.5):\n",
    "        preds = self.sigmoid(self.forward(x))\n",
    "        preds = np.array(preds.cpu() > threshold, dtype=float)\n",
    "        return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "parameters_dictionary = {}\n",
    "parameters = Parameters({'vocab_size': tokenizer.vocab_size, 'embed_dim': 300,\n",
    "                         'rnn_hidden_dim': 500, 'bidirectional': True, 'linear_dims': [128, 300, 14],\n",
    "                         'label_size': len(encoder.classes_), 'rnn_type': RnnType.LSTM, 'num_layers': 4,\n",
    "                         'dropout': 0.0, 'attention_model': AttentionModel.GENERAL}\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/30, Micro-f1: 0.713, Train Loss: 0.147, Dev Loss: 0.615"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30 # epoch\n",
    "LR = 0.001  # learning rate\n",
    "\n",
    "model = CNNClassifier_1(\n",
    "    tokenizer.vocab_size,\n",
    "    6\n",
    ")\n",
    "\n",
    "# model = RnnClassifier(\n",
    "#     torch.device(device),\n",
    "#     parameters\n",
    "# )\n",
    "\n",
    "# model = CNNClassifier_2(\n",
    "#     tokenizer.vocab_size,\n",
    "#     8\n",
    "# )\n",
    "model.to(device)\n",
    "\n",
    "loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data['input_ids'].to(device))\n",
    "        loss = loss_fun(outputs, data['label_1'].to(device))\n",
    "        loss.backward()\n",
    "        # print(model.linears[4].weight.grad)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dev_dataloader):\n",
    "            output_batch = model.predict(data['input_ids'].to(device))\n",
    "            target_batch = np.array(data['label_1'])\n",
    "            outputs.extend(output_batch)\n",
    "            targets.extend(target_batch)\n",
    "            # dev_dataloader[idx]['label_1_output'] = outputs\n",
    "\n",
    "    micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "    dev_loss = loss_fun(torch.FloatTensor(outputs), torch.FloatTensor(targets))\n",
    "    print(f'\\rEpoch: {epoch}/{EPOCHS}, Micro-f1: {micro_f1:.3f}, Train Loss: {epoch_loss/len(train_dataloader):.3f}, Dev Loss: {dev_loss:.3f}', end='')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.29      0.40        17\n",
      "           1       1.00      0.50      0.67        16\n",
      "           2       0.80      0.75      0.77        16\n",
      "           3       0.70      0.50      0.58        14\n",
      "           4       0.91      0.87      0.89        23\n",
      "           5       0.83      0.77      0.80        13\n",
      "\n",
      "   micro avg       0.83      0.63      0.71        99\n",
      "   macro avg       0.81      0.61      0.69        99\n",
      "weighted avg       0.82      0.63      0.70        99\n",
      " samples avg       0.76      0.61      0.65        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = SemEvalTask3Subtask2(\n",
    "    X_dev, Y_dev, tokenizer, labels_1=outputs\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(classification_report(targets, outputs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       210\n",
      "           1       1.00      1.00      1.00       105\n",
      "           2       1.00      1.00      1.00       187\n",
      "           3       1.00      1.00      1.00       189\n",
      "           4       1.00      1.00      1.00       212\n",
      "           5       1.00      1.00      1.00       174\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1077\n",
      "   macro avg       1.00      1.00      1.00      1077\n",
      "weighted avg       1.00      1.00      1.00      1077\n",
      " samples avg       0.95      0.95      0.95      1077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "train_outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        output_batch = model.predict(data['input_ids'].to(device))\n",
    "        target_batch = np.array(data['label_1'])\n",
    "        train_outputs.extend(output_batch)\n",
    "        targets.extend(target_batch)\n",
    "\n",
    "print(classification_report(targets, train_outputs))\n",
    "train_dataset = SemEvalTask3Subtask2(\n",
    "    X_train, Y_train, tokenizer, labels_1=train_outputs\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/30, Micro-f1: 0.300, Train Loss: 0.072, Dev Loss: 0.689"
     ]
    }
   ],
   "source": [
    "model2 = CNNClassifier_2(\n",
    "    tokenizer.vocab_size,\n",
    "    8\n",
    ")\n",
    "model2.to(device)\n",
    "\n",
    "loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=LR)\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    model2.train()\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(data['input_ids'].to(device), data['label_1_output'])\n",
    "        loss = loss_fun(outputs, data['label'].to(device))\n",
    "        loss.backward()\n",
    "        # print(model.linears[4].weight.grad)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model2.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dev_dataloader):\n",
    "            output_batch = model2.predict(data['input_ids'].to(device), data['label_1_output'])\n",
    "            target_batch = np.array(data['label'])\n",
    "            outputs.extend(output_batch)\n",
    "            targets.extend(target_batch)\n",
    "\n",
    "    micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "    dev_loss = loss_fun(torch.FloatTensor(outputs), torch.FloatTensor(targets))\n",
    "    print(f'\\rEpoch: {epoch}/{EPOCHS}, Micro-f1: {micro_f1:.3f}, Train Loss: {epoch_loss/len(train_dataloader):.3f}, Dev Loss: {dev_loss:.3f}', end='')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       1.00      0.12      0.22         8\n",
      "           4       1.00      0.50      0.67         2\n",
      "           5       0.00      0.00      0.00         2\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       1.00      0.33      0.50         3\n",
      "\n",
      "   micro avg       1.00      0.18      0.30        17\n",
      "   macro avg       0.38      0.12      0.17        17\n",
      "weighted avg       0.76      0.18      0.27        17\n",
      " samples avg       0.06      0.06      0.06        17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "outputs_dev2 = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(dev_dataloader):\n",
    "        output_batch = model2.predict(data['input_ids'].to(device), data['label_1_output'])\n",
    "        target_batch = np.array(data['label'])\n",
    "        outputs_dev2.extend(output_batch)\n",
    "        targets.extend(target_batch)\n",
    "\n",
    "print(classification_report(targets, outputs_dev2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        35\n",
      "           1       0.00      0.00      0.00        77\n",
      "           2       0.00      0.00      0.00        62\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        82\n",
      "           5       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       282\n",
      "   macro avg       0.00      0.00      0.00       282\n",
      "weighted avg       0.00      0.00      0.00       282\n",
      " samples avg       0.00      0.00      0.00       282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs_test = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        output_batch = model.predict(data['input_ids'].to(device), data['label_1_output'])\n",
    "        target_batch = np.array(data['label_1'])\n",
    "        outputs_test.extend(output_batch)\n",
    "        targets.extend(target_batch)\n",
    "\n",
    "print(classification_report(targets, outputs_test))\n",
    "\n",
    "test_dataset = SemEvalTask3Subtask2(\n",
    "    X_dev, Y_dev, tokenizer, labels_1=outputs_test\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00        16\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       0.00      0.00      0.00         3\n",
      "           5       0.00      0.00      0.00        61\n",
      "           6       0.00      0.00      0.00        29\n",
      "           7       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       138\n",
      "   macro avg       0.00      0.00      0.00       138\n",
      "weighted avg       0.00      0.00      0.00       138\n",
      " samples avg       0.00      0.00      0.00       138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jonibekmansurov/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "outputs_test2 = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(test_dataloader):\n",
    "        output_batch = model2.predict(data['input_ids'].to(device), data['label_1_output'])\n",
    "        target_batch = np.array(data['label'], )\n",
    "        outputs_test2.extend(output_batch)\n",
    "        targets.extend(target_batch)\n",
    "\n",
    "print(classification_report(targets, outputs_test2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " for idx, data in enumerate(train_dataloader):\n",
    "    print(data['label'])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(outputs_test2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected indicator for 14 classes, but got 8",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-247-717719fde56f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mencoder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minverse_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs_test2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/preprocessing/_label.py\u001B[0m in \u001B[0;36minverse_transform\u001B[0;34m(self, yt)\u001B[0m\n\u001B[1;32m    899\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    900\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0myt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclasses_\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 901\u001B[0;31m             raise ValueError(\n\u001B[0m\u001B[1;32m    902\u001B[0m                 \"Expected indicator for {0} classes, but got {1}\".format(\n\u001B[1;32m    903\u001B[0m                     \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclasses_\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Expected indicator for 14 classes, but got 8"
     ]
    }
   ],
   "source": [
    "encoder.inverse_transform(np.array(outputs_test2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import XLNetModel\n",
    "import torch.nn as nn\n",
    "class XLNet_Model(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(XLNet_Model, self).__init__()\n",
    "        self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "        self.out = nn.Linear(self.xlnet.config.hidden_size, classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs = self.xlnet(input)\n",
    "        out = self.out(outputs.last_hidden_state)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x, threshold=0.5):\n",
    "        preds = self.sigmoid(self.forward(x))\n",
    "        preds = np.array(preds.cpu() > threshold, dtype=float)\n",
    "        return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = XLNet_Model(14)\n",
    "model.to(device)\n",
    "\n",
    "loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # model.hidden = model.init_hidden(BATCH_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data['input_ids'].to(device))\n",
    "        loss = loss_fun(outputs, data['label'].to(device))\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        # print(model.grad)\n",
    "        # print(model.embs[0].weight.grad)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dev_dataloader):\n",
    "            output_batch = model.predict(data['input_ids'].to(device))\n",
    "            target_batch = np.array(data['label'])\n",
    "            outputs.extend(output_batch)\n",
    "            targets.extend(target_batch)\n",
    "\n",
    "    micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "    dev_loss = loss_fun(torch.FloatTensor(outputs), torch.FloatTensor(targets))\n",
    "    print(f'\\rEpoch: {epoch}/{EPOCHS}, Micro-f1: {micro_f1:.3f}, Train Loss: {epoch_loss/len(train_dataloader):.3f}, Dev Loss: {dev_loss:.3f}', end='')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}